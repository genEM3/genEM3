{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pigeon import annotate\n",
    "\n",
    "from genEM3.data.wkwdata import WkwData,DataSource\n",
    "from genEM3.util.path import get_data_dir\n",
    "from genEM3.data.annotation import update_data_source_bbox, update_data_source_targets, display_example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Loaded the json file for the dataset\n",
    "datasources_json_path = os.path.join(get_data_dir(), 'debris_clean_added_bboxes2_wiggle_datasource.json') \n",
    "input_shape = (140, 140, 1)\n",
    "output_shape = (140, 140, 1)\n",
    "cache_RAM = True\n",
    "cache_HDD = True\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "cache_HDD_root = os.path.join(get_data_dir(), '.cache/')\n",
    "\n",
    "data_sources = WkwData.datasources_from_json(datasources_json_path)\n",
    "\n",
    "dataset = WkwData(\n",
    "    input_shape=input_shape,\n",
    "    target_shape=output_shape,\n",
    "    data_sources=data_sources,\n",
    "    cache_RAM=cache_RAM,\n",
    "    cache_HDD=cache_HDD,\n",
    "    cache_HDD_root=cache_HDD_root\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a set of data sources with the normal bounding boxes to create a patch wise detaset and a larger bounding box for annotation\n",
    "margin = 35\n",
    "corner_xy_index = [0,1]\n",
    "length_xy_index = [3,4]\n",
    "roi_size = 140\n",
    "large_bboxes_idx = []\n",
    "bboxes_idx = []\n",
    "for idx in range(len(dataset)):\n",
    "    (source_idx, original_cur_bbox) = dataset.get_bbox_for_sample_idx(idx)\n",
    "    bboxes_idx.append((source_idx, original_cur_bbox))\n",
    "    cur_bbox = np.asarray(original_cur_bbox)\n",
    "    cur_bbox[corner_xy_index] = cur_bbox[corner_xy_index] - margin\n",
    "    cur_bbox[length_xy_index] = cur_bbox[length_xy_index] + margin*2\n",
    "    # large bbox append\n",
    "    large_bboxes_idx.append((source_idx, cur_bbox.tolist()))\n",
    "    \n",
    "assert len(large_bboxes_idx) == len(dataset) == len(bboxes_idx)\n",
    "larger_sources = update_data_source_bbox(dataset, large_bboxes_idx)\n",
    "patch_source_list = update_data_source_bbox(dataset, bboxes_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with updated data source for annotation\n",
    "input_shape = tuple(large_bboxes_idx[0][1][3:6])\n",
    "larger_dataset = WkwData(\n",
    "    input_shape=input_shape,\n",
    "    target_shape=input_shape,\n",
    "    data_sources=larger_sources,\n",
    "    cache_RAM=cache_RAM,\n",
    "    cache_HDD=cache_HDD,\n",
    "    cache_HDD_root=cache_HDD_root\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(bboxes_idx[0][1][3:6])\n",
    "patch_dataset = WkwData(\n",
    "    input_shape=input_shape,\n",
    "    target_shape=input_shape,\n",
    "    data_sources=patch_source_list,\n",
    "    cache_RAM=cache_RAM,\n",
    "    cache_HDD=cache_HDD,\n",
    "    cache_HDD_root=cache_HDD_root\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patch_dataset.data_sources[-1])\n",
    "print(patch_source_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down the range into partitions of 1000\n",
    "range_size = 1000\n",
    "num_thousand, remainder = divmod(len(larger_dataset), range_size)\n",
    "list_ranges = []\n",
    "# Create a list of ranges\n",
    "for i in range(num_thousand):\n",
    "    list_ranges.append(range(i*range_size, (i+1)*range_size))\n",
    "if remainder > 0:\n",
    "    final_range = range(num_thousand*range_size, num_thousand*range_size+remainder)\n",
    "    list_ranges.append(final_range)\n",
    "\n",
    "print(list_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annotate data using pigeon\n",
    "annotation_fun = lambda i: display_example(i, dataset=larger_dataset, margin=margin, roi_size=roi_size)\n",
    "annotations = []\n",
    "for cur_range in list_ranges:\n",
    "    print(f'Following range is {cur_range}')\n",
    "    cur_a = annotate(cur_range,\n",
    "                     options=['clean', 'debris', 'myelin'],\n",
    "                     display_fn=annotation_fun)\n",
    "    annotations.append(cur_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save annotations\n",
    "fname = 'annotationList_original_v04.pkl'\n",
    "with open(fname, 'wb') as fp:\n",
    "    pickle.dump(annotations, fp)\n",
    "\n",
    "# Test reading it backabs\n",
    "with open (fname, 'rb') as fp:\n",
    "    annotations_reloaded = pickle.load(fp)\n",
    "    \n",
    "assert annotations == annotations_reloaded\n",
    "print(annotations_reloaded[8][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one list with the concatenation of individual batches of ~1000\n",
    "annotations_list = list(itertools.chain.from_iterable(annotations))\n",
    "# Check that the indices in the list are a continuous range\n",
    "assert [a for (a, _) in annotations_list ] == list(range(len(annotations_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the annotations to numbers\n",
    "types = ['clean', 'debris', 'myelin']\n",
    "name_to_target = {'clean': 0.0, 'debris': 1.0, 'myelin': 0.0}\n",
    "\n",
    "index_target_tuples = [(a[0], name_to_target[a[1]]) for a in annotations_list]\n",
    "source_list = update_data_source_targets(patch_dataset, index_target_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_list[-1])\n",
    "print(patch_dataset.data_sources[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json name\n",
    "json_name = os.path.join(get_data_dir(), 'debris_clean_added_bboxes2_wiggle_datasource_without_myelin_v01.json')\n",
    "# Write to json file\n",
    "WkwData.datasources_to_json(source_list, json_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge json files to get the training data\n",
    "from genEM3.data.annotation import merge_json_from_data_dir\n",
    "fnames_with = ['original_dataset_8562_patches/debris_clean_added_bboxes2_wiggle_datasource_without_myelin_v01.json', \n",
    "               'dense_3X_10_10_2_um/test_data_three_bboxes_without_myelin_v01.json']\n",
    "output = 'dense_3X_10_10_2_um/original_merged_without_myelin_v01.json'\n",
    "all_data_sources = merge_json_from_data_dir(fnames=fnames_with, output_fname=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at individual examples\n",
    "test_target = [(i,t) for (i,t) in index_target_tuples if int(t)==1]\n",
    "for (i,t) in test_target:\n",
    "    print(f'sample index: {i}, AK: {types[int(t)]}')\n",
    "    annotation_fun(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare targets annotated by me and Flo\n",
    "name_to_target = {'clean': 0, 'debris': 1, 'myelin': 0}\n",
    "targets_AK = [name_to_target[a[1]] for a in annotations]\n",
    "targets_Flo = [int(dataset.get_target_from_sample_idx(i)) for i in range(len(dataset))]\n",
    "\n",
    "list_tuples = list(zip(targets_AK, targets_Flo))\n",
    "\n",
    "agreement_list = [int(l[0] == l[1]) for l in list_tuples]\n",
    "\n",
    "print(f'The number of disagreements: {len(agreement_list) - sum(agreement_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the disagreements\n",
    "index_disagreement = [i for i, cond in enumerate(agreement_list) if not cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the disagreements to an NML file\n",
    "from genEM3.data.skeleton import add_bbox_tree\n",
    "from wkskel import Skeleton\n",
    "from genEM3.util.path import get_runs_dir\n",
    "\n",
    "types = ['clean', 'debris']\n",
    "skel = Skeleton(os.path.join(get_runs_dir(), 'inference/ae_classify_11_parallel/empty.nml'))\n",
    "input_shape = (140, 140, 1)\n",
    "# Write to nml\n",
    "for i in index_disagreement:\n",
    "    tree_name = f'sample index: {i}, AK: {types[targets_AK[i]]}, Flo: {types[targets_Flo[i]]}, your opinion:'\n",
    "    sample_center = dataset.get_center_for_sample_idx(i)\n",
    "    add_bbox_tree(sample_center, input_shape, tree_name, skel)\n",
    "    \n",
    "skel.write_nml(os.path.join(get_data_dir(), 'test_dataset_annotation_disagreement_v01.nml'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}