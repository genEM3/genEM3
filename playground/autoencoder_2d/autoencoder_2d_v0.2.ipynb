{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/2016-05-26_st015_000303_000_000_000_n_00_crop_000.tif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0cdbab56cc76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's look at some data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2016-05-26_st015_000303_000_000_000_n_00_crop_000.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/anaconda3/envs/gen-EM/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/anaconda3/envs/gen-EM/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1415\u001b[0m                              \u001b[0;34m'with Pillow installed matplotlib can handle '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                              'more images' % list(handlers))\n\u001b[0;32m-> 1417\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/anaconda3/envs/gen-EM/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/2016-05-26_st015_000303_000_000_000_n_00_crop_000.tif'"
     ]
    }
   ],
   "source": [
    "# Let's look at some data\n",
    "img = plt.imread(os.path.join(data_root, '2016-05-26_st015_000303_000_000_000_n_00_crop_000.tif'))\n",
    "plt.imshow(img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a quick-and-dirty estimate of image summary stats\n",
    "norm_params = {'mean': np.mean(img), 'std': np.std(img)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBEMCrop2dDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, norm_params):\n",
    "        self.data_fnames = os.listdir(data_root)\n",
    "        self.data_root = data_root\n",
    "        self.norm_params = norm_params\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = np.asarray(plt.imread(os.path.join(self.data_root, self.data_fnames[idx])))\n",
    "        img = self._normalize(img)\n",
    "        sample = SBEMCrop2dDataset._reshape_to_torch(img)\n",
    "        return sample\n",
    "    \n",
    "    def _normalize(self, img):\n",
    "        img = (np.asarray(img)-self.norm_params['mean'])/self.norm_params['std']\n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reshape_to_torch(img):\n",
    "        sample = torch.from_numpy(np.reshape(img, (1, img.shape[0], img.shape[1]))).float()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate dataset subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbem_dataset = SBEMCrop2dDataset(data_root, norm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 images in dataset\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16,12))\n",
    "\n",
    "for i in range(3):\n",
    "    sample = sbem_dataset[i]\n",
    "    img = sample.data.numpy().squeeze()\n",
    "    axs[i].imshow(img, cmap='gray')\n",
    "    axs[i].set_title('mean: {:0.3f}, std: {:0.3f}'.format(np.mean(img), np.std(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAE2D, self).__init__()\n",
    "        self.encoding_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU())\n",
    "        self.encoding_layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 8, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU())\n",
    "        self.encoding_layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(8, 4, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU())\n",
    "        self.decoding_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(4, 8, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU())\n",
    "        self.decoding_layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(8, 16, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.ReLU())\n",
    "        self.decoding_layer3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(16, 1, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoding_layer1(x)\n",
    "        x = self.encoding_layer2(x)\n",
    "        x = self.encoding_layer3(x)\n",
    "        x = self.decoding_layer1(x)\n",
    "        x = self.decoding_layer2(x)\n",
    "        x = self.decoding_layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE2D_2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAE2D_2, self).__init__()\n",
    "        self.encoding_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "        self.encoding_pool1 = torch.nn.MaxPool2d(2)\n",
    "        self.encoding_layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "        self.encoding_pool2 = torch.nn.MaxPool2d(2)\n",
    "        self.encoding_layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "        self.decoding_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(4, 8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "        self.decoding_up1 = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.decoding_layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "        self.decoding_up2 = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.decoding_layer3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=0),\n",
    "            torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoding_layer1(x)\n",
    "        x = self.encoding_pool1(x)\n",
    "        x = self.encoding_layer2(x)\n",
    "        x = self.encoding_pool2(x)\n",
    "        x = self.encoding_layer3(x)\n",
    "        x = self.decoding_layer1(x)\n",
    "        x = self.decoding_up1(x)\n",
    "        x = self.decoding_layer2(x)\n",
    "        x = self.decoding_up2(x)\n",
    "        x = self.decoding_layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvAE2D_2()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(sbem_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "n_epoch = 30\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data\n",
    "        labels = inputs\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show input vs output\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16,12))\n",
    "\n",
    "img_input = inputs[0].data.numpy().squeeze()\n",
    "axs[0].imshow(img_input, cmap='gray')\n",
    "\n",
    "img_output = outputs[0].data.numpy().squeeze()\n",
    "axs[1].imshow(img_output, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
